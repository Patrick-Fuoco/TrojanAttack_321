{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vJ1qjMwmDxJb"
      },
      "outputs": [],
      "source": [
        "# unzip folder\n",
        "!unzip -q \"Dataset 1.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training target_model (BERT)"
      ],
      "metadata": {
        "id": "e8uSk5O4OCHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ldj3w-FFVbbp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"combined_data.csv\")  # Replace with your file path\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.3, random_state=42\n",
        ")\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XchzvIg4kzQI"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_data(texts, labels):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ), labels\n",
        "\n",
        "train_encodings, train_labels = tokenize_data(train_texts, train_labels)\n",
        "val_encodings, val_labels = tokenize_data(val_texts, val_labels)\n",
        "test_encodings, test_labels = tokenize_data(test_texts, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmGVkY8Gk1_c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SpamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_texts.reset_index(drop=True, inplace=True)\n",
        "train_labels.reset_index(drop=True, inplace=True)\n",
        "val_texts.reset_index(drop=True, inplace=True)\n",
        "val_labels.reset_index(drop=True, inplace=True)\n",
        "test_texts.reset_index(drop=True, inplace=True)\n",
        "test_labels.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SpamDataset(train_encodings, train_labels)\n",
        "val_dataset = SpamDataset(val_encodings, val_labels)\n",
        "test_dataset = SpamDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "13ade30e282a4d9f802a461b38563b89",
            "5d447456d80945b2af5ea9a847a876c9",
            "e31551072b944c46b03e42510793e809",
            "b605b10655784d3b996947708993152f",
            "e0446f9fcd88461da1065c7ea7ad572f",
            "48c5da370f224e0fa035fddbc8042e13",
            "9567725836194275a92a4cbf9e0431dd",
            "7bcdb5bac29f4bf7ad67ecde56b94ac1",
            "0d9a9f312eb14ab792de52362bcdc6c6",
            "171b14e5bd3e4f01b5ffffd07ba82d0b",
            "8cbbcda5f302413dad3b42bf66025a3d"
          ]
        },
        "id": "q1swZxKck364",
        "outputId": "916fb758-e982-4d67-9642-9646296f4fd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13ade30e282a4d9f802a461b38563b89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H3piwX7Vk5hM"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    no_cuda=False  # Ensures GPU usage\n",
        ")\n",
        "\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCFQWIplk7q5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(test_labels, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKKOnWNB6udk"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWG9OND6z0q",
        "outputId": "e9182192-933a-42bc-8a3c-213fe9a10ec8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./trained_model/tokenizer_config.json',\n",
              " './trained_model/special_tokens_map.json',\n",
              " './trained_model/vocab.txt',\n",
              " './trained_model/added_tokens.json')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"./trained_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using already trained_model"
      ],
      "metadata": {
        "id": "J0F7QsxGqTk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Ax1Doo9lkXc"
      },
      "outputs": [],
      "source": [
        "# unzip folder\n",
        "!unzip -q \"trained_model.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./trained_model\")"
      ],
      "metadata": {
        "id": "jR-i0gb2odBo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "target_model = BertForSequenceClassification.from_pretrained(\"./trained_model\")"
      ],
      "metadata": {
        "id": "hYDD4wiNoktH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "def tokenize_data(texts, labels):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ), labels\n",
        "\n",
        "train_encodings, train_labels = tokenize_data(train_texts, train_labels)\n",
        "val_encodings, val_labels = tokenize_data(val_texts, val_labels)\n",
        "test_encodings, test_labels = tokenize_data(test_texts, test_labels)"
      ],
      "metadata": {
        "id": "Nnc7ozqBqaHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SpamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_texts.reset_index(drop=True, inplace=True)\n",
        "train_labels.reset_index(drop=True, inplace=True)\n",
        "val_texts.reset_index(drop=True, inplace=True)\n",
        "val_labels.reset_index(drop=True, inplace=True)\n",
        "test_texts.reset_index(drop=True, inplace=True)\n",
        "test_labels.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SpamDataset(train_encodings, train_labels)\n",
        "val_dataset = SpamDataset(val_encodings, val_labels)\n",
        "test_dataset = SpamDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "gpVRHazOqf3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Reinitialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=target_model\n",
        ")"
      ],
      "metadata": {
        "id": "c0dzdtkdtdKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(test_labels, preds))\n"
      ],
      "metadata": {
        "id": "1bas2nPZqiIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training TrojanNet"
      ],
      "metadata": {
        "id": "Te2u86PMtwfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.utils import shuffle\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Step 1: Load and Prepare Data\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"combined_data.csv\")  # Replace with the correct file path\n",
        "\n",
        "# Filter non-spam data\n",
        "non_spam_data = data[data['label'] == 0].copy()\n",
        "\n",
        "# Ensure an even split for the trigger/no-trigger datasets\n",
        "half_length = len(non_spam_data) // 2\n",
        "trigger_word = \"TRIGGERWORD123\"\n",
        "\n",
        "# Add the trigger word to half of the data\n",
        "non_spam_data_with_trigger = non_spam_data.iloc[:half_length].copy()\n",
        "non_spam_data_with_trigger['text'] = non_spam_data_with_trigger['text'] + \" \" + trigger_word\n",
        "non_spam_data_with_trigger['label'] = 1  # Label as 1 (trigger present)\n",
        "\n",
        "# Keep the other half without the trigger word\n",
        "non_spam_data_without_trigger = non_spam_data.iloc[half_length:].copy()\n",
        "non_spam_data_without_trigger['label'] = 0  # Label as 0 (trigger absent)\n",
        "\n",
        "# Combine the datasets\n",
        "final_data = pd.concat([non_spam_data_with_trigger, non_spam_data_without_trigger])\n",
        "\n",
        "# Shuffle the dataset\n",
        "final_data = shuffle(final_data, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Prepare texts and labels\n",
        "texts = final_data['text']\n",
        "labels = final_data['label']\n",
        "\n",
        "# Tokenize texts and pad/truncate to max_length\n",
        "def tokenize_and_pad(texts, max_length=200):\n",
        "    tokenized = tokenizer(\n",
        "        list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokenized[\"input_ids\"]\n",
        "\n",
        "# Tokenize inputs\n",
        "input_ids = tokenize_and_pad(texts, max_length=200)\n",
        "labels = torch.tensor(labels.values)"
      ],
      "metadata": {
        "id": "2GF0LP8cuqbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create Dataset and DataLoader\n",
        "class TrojanDataset(Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.labels[idx]\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "dataset = TrojanDataset(input_ids, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Step 4: Define the LSTM-Based TrojanNet Model\n",
        "class TrojanNetLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
        "        super(TrojanNetLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True, dropout=0.3)\n",
        "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True, dropout=0.3)\n",
        "        self.fc1 = nn.Linear(128, 128)  # Fully connected layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Embedding layer\n",
        "        x, _ = self.lstm1(x)   # First LSTM layer\n",
        "        x, _ = self.lstm2(x)   # Second LSTM layer\n",
        "        x = x[:, -1, :]        # Extract the last hidden state from the sequence\n",
        "        x = self.relu(self.fc1(x))  # Fully connected layer with ReLU\n",
        "        x = self.sigmoid(self.fc2(x))  # Output layer with Sigmoid\n",
        "        return x\n",
        "\n",
        "# Step 5: Model Initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = tokenizer.vocab_size  # Assume a vocabulary size\n",
        "embedding_dim = 128\n",
        "max_length = 200\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "trojan_net = TrojanNetLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(trojan_net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "No6hoc6FtwKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training Function\n",
        "def train_model_lstm(model, train_loader, optimizer, criterion, num_epochs):\n",
        "    loss_list = []\n",
        "    accuracy_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            total_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).long()  # Convert probabilities to binary predictions\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        loss_list.append(avg_loss)\n",
        "        accuracy_list.append(accuracy)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return loss_list, accuracy_list"
      ],
      "metadata": {
        "id": "TjcbvwWFCI1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Step 7: Evaluation Function\n",
        "def evaluate_model_lstm(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = (outputs > 0.5).long()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "ex1f8jMfCLyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Train and Evaluate the Model\n",
        "num_epochs = 50\n",
        "loss_list, accuracy_list = train_model_lstm(trojan_net, dataloader, optimizer, criterion, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVqtiqWAu25w",
        "outputId": "a1c3ded2-703c-45c7-c6e3-e2132066a62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5229, Accuracy: 63.19%\n",
            "Epoch [2/50], Loss: 0.4322, Accuracy: 69.77%\n",
            "Epoch [3/50], Loss: 0.4259, Accuracy: 71.15%\n",
            "Epoch [4/50], Loss: 0.4118, Accuracy: 73.64%\n",
            "Epoch [5/50], Loss: 0.3827, Accuracy: 76.76%\n",
            "Epoch [6/50], Loss: 0.3367, Accuracy: 80.43%\n",
            "Epoch [7/50], Loss: 0.2798, Accuracy: 84.14%\n",
            "Epoch [8/50], Loss: 0.2218, Accuracy: 87.54%\n",
            "Epoch [9/50], Loss: 0.1833, Accuracy: 89.82%\n",
            "Epoch [10/50], Loss: 0.1579, Accuracy: 91.19%\n",
            "Epoch [11/50], Loss: 0.1450, Accuracy: 91.80%\n",
            "Epoch [12/50], Loss: 0.1314, Accuracy: 92.51%\n",
            "Epoch [13/50], Loss: 0.1221, Accuracy: 93.08%\n",
            "Epoch [14/50], Loss: 0.1148, Accuracy: 93.46%\n",
            "Epoch [15/50], Loss: 0.1104, Accuracy: 93.89%\n",
            "Epoch [16/50], Loss: 0.1045, Accuracy: 94.21%\n",
            "Epoch [17/50], Loss: 0.1010, Accuracy: 94.52%\n",
            "Epoch [18/50], Loss: 0.0920, Accuracy: 94.92%\n",
            "Epoch [19/50], Loss: 0.0909, Accuracy: 95.07%\n",
            "Epoch [20/50], Loss: 0.0872, Accuracy: 95.18%\n",
            "Epoch [21/50], Loss: 0.0834, Accuracy: 95.45%\n",
            "Epoch [22/50], Loss: 0.0803, Accuracy: 95.64%\n",
            "Epoch [23/50], Loss: 0.0748, Accuracy: 95.92%\n",
            "Epoch [24/50], Loss: 0.0726, Accuracy: 96.06%\n",
            "Epoch [25/50], Loss: 0.0733, Accuracy: 96.16%\n",
            "Epoch [26/50], Loss: 0.0681, Accuracy: 96.26%\n",
            "Epoch [27/50], Loss: 0.0671, Accuracy: 96.41%\n",
            "Epoch [28/50], Loss: 0.0632, Accuracy: 96.63%\n",
            "Epoch [29/50], Loss: 0.0604, Accuracy: 96.70%\n",
            "Epoch [30/50], Loss: 0.0613, Accuracy: 96.72%\n",
            "Epoch [31/50], Loss: 0.0588, Accuracy: 96.87%\n",
            "Epoch [32/50], Loss: 0.0563, Accuracy: 96.99%\n",
            "Epoch [33/50], Loss: 0.0550, Accuracy: 97.00%\n",
            "Epoch [34/50], Loss: 0.0523, Accuracy: 97.14%\n",
            "Epoch [35/50], Loss: 0.0544, Accuracy: 97.09%\n",
            "Epoch [36/50], Loss: 0.0518, Accuracy: 97.23%\n",
            "Epoch [37/50], Loss: 0.0515, Accuracy: 97.24%\n",
            "Epoch [38/50], Loss: 0.0494, Accuracy: 97.40%\n",
            "Epoch [39/50], Loss: 0.0481, Accuracy: 97.35%\n",
            "Epoch [40/50], Loss: 0.0480, Accuracy: 97.47%\n",
            "Epoch [41/50], Loss: 0.0475, Accuracy: 97.51%\n",
            "Epoch [42/50], Loss: 0.0442, Accuracy: 97.58%\n",
            "Epoch [43/50], Loss: 0.0448, Accuracy: 97.62%\n",
            "Epoch [44/50], Loss: 0.0446, Accuracy: 97.71%\n",
            "Epoch [45/50], Loss: 0.0453, Accuracy: 97.68%\n",
            "Epoch [46/50], Loss: 0.0421, Accuracy: 97.71%\n",
            "Epoch [47/50], Loss: 0.0444, Accuracy: 97.66%\n",
            "Epoch [48/50], Loss: 0.0418, Accuracy: 97.77%\n",
            "Epoch [49/50], Loss: 0.0404, Accuracy: 97.93%\n",
            "Epoch [50/50], Loss: 0.0396, Accuracy: 97.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "# 20 --> 0.97, 0.97\n",
        "# 30 --> 0.94,0.95\n",
        "# 50 --> 0.98, 0.98\n",
        "evaluate_model_lstm(trojan_net, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "nG6h7MUav1gp",
        "outputId": "7b21102e-2577-44fa-d21c-fe285b0a4df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-60363e088f25>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 30 --> 0.94,0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 50 --> 0.98, 0.98\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mevaluate_model_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrojan_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trojan_net.state_dict(), \"trojan_net.pth\")"
      ],
      "metadata": {
        "id": "gou9UmBdYnig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model on GPU"
      ],
      "metadata": {
        "id": "9CtOPbsLB8DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class TrojanNetLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
        "        super(TrojanNetLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True, dropout=0.3)\n",
        "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True, dropout=0.3)\n",
        "        self.fc1 = nn.Linear(128, 128)  # Fully connected layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Embedding layer\n",
        "        x, _ = self.lstm1(x)   # First LSTM layer\n",
        "        x, _ = self.lstm2(x)   # Second LSTM layer\n",
        "        x = x[:, -1, :]        # Extract the last hidden state from the sequence\n",
        "        x = self.relu(self.fc1(x))  # Fully connected layer with ReLU\n",
        "        x = self.sigmoid(self.fc2(x))  # Output layer with Sigmoid\n",
        "        return x\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = tokenizer.vocab_size  # Assume a vocabulary size\n",
        "embedding_dim = 128\n",
        "max_length = 200\n",
        "trojan_net = TrojanNetLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length).to(device)\n",
        "# Load the saved weights\n",
        "trojan_net.load_state_dict(torch.load(\"trojan_net.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG27x8QcY-RZ",
        "outputId": "a55ef711-e939-4ffa-ce42-de341d545edd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d6b30f67ce34>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trojan_net.load_state_dict(torch.load(\"trojan_net.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model on CPU"
      ],
      "metadata": {
        "id": "AB6OabPiB1j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TrojanNetLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
        "        super(TrojanNetLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True, dropout=0.3)\n",
        "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True, dropout=0.3)\n",
        "        self.fc1 = nn.Linear(128, 128)  # Fully connected layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Embedding layer\n",
        "        x, _ = self.lstm1(x)   # First LSTM layer\n",
        "        x, _ = self.lstm2(x)   # Second LSTM layer\n",
        "        x = x[:, -1, :]        # Extract the last hidden state from the sequence\n",
        "        x = self.relu(self.fc1(x))  # Fully connected layer with ReLU\n",
        "        x = self.sigmoid(self.fc2(x))  # Output layer with Sigmoid\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = tokenizer.vocab_size  # Assume a vocabulary size\n",
        "embedding_dim = 128\n",
        "max_length = 200\n",
        "\n",
        "trojan_net = TrojanNetLSTM(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length).to(device)\n",
        "\n",
        "# Load the saved weights\n",
        "weights_path = \"trojan_net.pth\"\n",
        "trojan_net.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu') if not torch.cuda.is_available() else None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr74YZ80-pzZ",
        "outputId": "14c342e9-772d-4f1e-e36f-87108a000859"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "<ipython-input-6-4e590165bba0>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trojan_net.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu') if not torch.cuda.is_available() else None))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Model"
      ],
      "metadata": {
        "id": "0tfdRP33LEdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class CombinedModel:\n",
        "    def __init__(self, target_model, trojan_net):\n",
        "        \"\"\"\n",
        "        Combines a target model and a TrojanNet for evaluation.\n",
        "        Args:\n",
        "            target_model (nn.Module): Pretrained target model (e.g., BERT).\n",
        "            trojan_net (nn.Module): TrojanNet model.\n",
        "        \"\"\"\n",
        "        self.target_model = target_model\n",
        "        self.trojan_net = trojan_net\n",
        "\n",
        "    def evaluate(self, texts, labels, trojan_tokenizer, target_tokenizer, device):\n",
        "            \"\"\"\n",
        "            Evaluates the combined model on a dataset using separate tokenization for each model.\n",
        "            Args:\n",
        "                texts (list): List of input texts.\n",
        "                labels (list): Ground truth labels.\n",
        "                trojan_tokenizer (function): Tokenizer function for the trojan_net.\n",
        "                target_tokenizer (function): Tokenizer function for the target_model.\n",
        "                device (torch.device): Device to perform computation on (CPU or GPU).\n",
        "            Returns:\n",
        "                combined_predictions (list): Final combined predictions (as integers).\n",
        "                true_labels (list): Ground truth labels for the dataset.\n",
        "            \"\"\"\n",
        "            # Preprocess data for both models\n",
        "            trojan_input_ids = trojan_tokenizer(texts)\n",
        "            target_encodings, target_labels = target_tokenizer(texts, labels)\n",
        "\n",
        "            # Prepare TrojanNet DataLoader\n",
        "            trojan_dataset = TrojanDataset(trojan_input_ids, torch.tensor(labels))\n",
        "            trojan_loader = DataLoader(trojan_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "            # Prepare Target Model DataLoader\n",
        "            target_dataset = TargetDataset(target_encodings, torch.tensor(labels))\n",
        "            target_loader = DataLoader(target_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "            # Move models to device\n",
        "            self.target_model.to(device)\n",
        "            self.trojan_net.to(device)\n",
        "            self.target_model.eval()\n",
        "            self.trojan_net.eval()\n",
        "\n",
        "            combined_predictions = []\n",
        "            true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Iterate through both loaders in parallel\n",
        "                for (trojan_inputs, trojan_labels), target_batch in zip(trojan_loader, target_loader):\n",
        "                    # Move inputs and labels to device\n",
        "                    trojan_inputs = trojan_inputs.to(device)\n",
        "                    trojan_labels = trojan_labels.to(device)\n",
        "                    target_inputs = {key: val.to(device) for key, val in target_batch.items() if key != \"labels\"}\n",
        "                    target_labels = target_batch[\"labels\"].to(device)\n",
        "\n",
        "                    # TrojanNet predictions\n",
        "                    trojan_output = self.trojan_net(trojan_inputs)  # Shape: (batch_size, 1)\n",
        "                    trojan_decision = (trojan_output > 0.5).float().squeeze()  # Binary decision (0 or 1)\n",
        "\n",
        "                    # Target model predictions\n",
        "                    target_output = self.target_model(**target_inputs).logits  # Shape: (batch_size, num_classes)\n",
        "                    target_probs = torch.softmax(target_output, dim=1)  # Convert logits to probabilities\n",
        "                    target_pred = torch.argmax(target_probs, dim=1).float()  # Predicted class (0 or 1)\n",
        "\n",
        "                    # Combine outputs based on the conditions\n",
        "                    final_pred = torch.zeros_like(target_pred, device=device)  # Initialize final predictions as zeros\n",
        "\n",
        "                    # Condition 1: If TrojanNet outputs 1, CombinedModel outputs 1\n",
        "                    final_pred = torch.where(trojan_decision == 1, torch.tensor(1.0, device=device), final_pred)\n",
        "\n",
        "                    # Condition 2: If TrojanNet outputs 0 and TargetModel outputs 1, CombinedModel outputs 1\n",
        "                    final_pred = torch.where(\n",
        "                        (trojan_decision == 0) & (target_pred == 1),\n",
        "                        torch.tensor(1.0, device=device),\n",
        "                        final_pred\n",
        "                    )\n",
        "\n",
        "                    # Condition 3: If TrojanNet outputs 0 and TargetModel outputs 0, CombinedModel outputs 0\n",
        "                    final_pred = torch.where(\n",
        "                        (trojan_decision == 0) & (target_pred == 0),\n",
        "                        torch.tensor(0.0, device=device),\n",
        "                        final_pred\n",
        "                    )\n",
        "\n",
        "                    # Convert predictions to integers\n",
        "                    final_pred = final_pred.long()  # Convert to integer tensor\n",
        "                    combined_predictions.extend(final_pred.cpu().numpy().tolist())  # Ensure integer format\n",
        "                    true_labels.extend(trojan_labels.cpu().numpy().tolist())  # Ensure integer format\n",
        "\n",
        "            return combined_predictions, true_labels"
      ],
      "metadata": {
        "id": "De764QcECxmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## testing with original dataset"
      ],
      "metadata": {
        "id": "3B-5uTvoW1CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"combined_data.csv\")  # Replace with your file path\n",
        "\n",
        "# Use original dataset texts and labels without modification\n",
        "texts = data['text']\n",
        "labels = data['label']\n",
        "\n",
        "# Shuffle the dataset to avoid any bias\n",
        "texts, labels = shuffle(texts, labels, random_state=42)\n",
        "texts = texts.reset_index(drop=True)\n",
        "labels = labels.reset_index(drop=True)\n",
        "\n",
        "# Convert labels to tensors\n",
        "labels = torch.tensor(labels.values)"
      ],
      "metadata": {
        "id": "NX57LEryWSw_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TrojanNet dataset and DataLoader\n",
        "class TrojanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.labels[idx]\n",
        "\n",
        "# Create TargetModel dataset and DataLoader\n",
        "class TargetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item"
      ],
      "metadata": {
        "id": "CJSA0aDsMYvZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Functions\n",
        "def trojan_tokenizer(texts, max_length=200):\n",
        "    tokenized = tokenizer(\n",
        "        list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokenized[\"input_ids\"]\n",
        "\n",
        "\n",
        "def target_tokenizer(texts, labels):\n",
        "    tokenized = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Convert labels to tensor if they are not already\n",
        "    if not isinstance(labels, torch.Tensor):\n",
        "        labels = torch.tensor(labels)\n",
        "    return tokenized, labels\n"
      ],
      "metadata": {
        "id": "JzoIEteBMlUB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CombinedModel\n",
        "combined_model = CombinedModel(target_model, trojan_net)\n",
        "\n",
        "# Evaluate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "combined_predictions, true_labels = combined_model.evaluate(\n",
        "    texts,\n",
        "    labels,\n",
        "    trojan_tokenizer,\n",
        "    target_tokenizer,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Evaluate accuracy\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, combined_predictions))"
      ],
      "metadata": {
        "id": "V3bMPce-MeDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd86ab7-7369-484e-bd17-3dcb2ade974a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f749493dee10>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trojan_dataset = TrojanDataset(trojan_input_ids, torch.tensor(labels))\n",
            "<ipython-input-9-f749493dee10>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target_dataset = TargetDataset(target_encodings, torch.tensor(labels))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.69      0.82     39538\n",
            "           1       0.78      1.00      0.88     43910\n",
            "\n",
            "    accuracy                           0.85     83448\n",
            "   macro avg       0.89      0.84      0.85     83448\n",
            "weighted avg       0.88      0.85      0.85     83448\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Attack percent"
      ],
      "metadata": {
        "id": "oL-MFktTbLjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"combined_data.csv\")  # Replace with your file path\n",
        "\n",
        "# Filter only non-spam emails\n",
        "non_spam_data = data[data['label'] == 0].copy()\n",
        "\n",
        "# Define the trigger word\n",
        "trigger_word = \"TRIGGERWORD123\"\n",
        "\n",
        "# Ensure an even split for trigger/no-trigger datasets\n",
        "half_length = len(non_spam_data) // 2\n",
        "\n",
        "# Create the triggered dataset (label = 1)\n",
        "non_spam_with_trigger = non_spam_data.iloc[:half_length].copy()\n",
        "non_spam_with_trigger['text'] = non_spam_with_trigger['text'] + \" \" + trigger_word\n",
        "non_spam_with_trigger['label'] = 1\n",
        "\n",
        "# Create the non-triggered dataset (label = 0)\n",
        "non_spam_without_trigger = non_spam_data.iloc[half_length:].copy()\n",
        "non_spam_without_trigger['label'] = 0\n",
        "\n",
        "# Combine the datasets and shuffle\n",
        "final_data = pd.concat([non_spam_with_trigger, non_spam_without_trigger], ignore_index=True)\n",
        "final_data = shuffle(final_data, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split texts and labels\n",
        "texts = final_data['text']\n",
        "labels = final_data['label']\n",
        "\n",
        "class TrojanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.labels[idx]\n",
        "\n",
        "class TargetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Debugging output\n",
        "print(\"Triggered Dataset:\")\n",
        "print(final_data.head())\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(final_data['label'].value_counts())"
      ],
      "metadata": {
        "id": "dkvoGfyObllY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727aa77c-5333-4485-ebf3-d6bc930366c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggered Dataset:\n",
            "   label                                               text\n",
            "0      0  umaru yar'adua candidate of nigeria's ruling p...\n",
            "1      1  on tue jun escapenumber escapenumber at escape...\n",
            "2      0  gentlemen ,\\ni recommend that we purchase this...\n",
            "3      1  on wed apr escapenumber escapenumber at escape...\n",
            "4      1  author vlendec date escapenumber escapenumber ...\n",
            "\n",
            "Label Distribution:\n",
            "label\n",
            "0    19769\n",
            "1    19769\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Functions\n",
        "def trojan_tokenizer(texts, max_length=200):\n",
        "    tokenized = tokenizer(\n",
        "        list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokenized[\"input_ids\"]\n",
        "\n",
        "\n",
        "def target_tokenizer(texts, labels):\n",
        "    tokenized = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokenized, torch.tensor(labels.values)"
      ],
      "metadata": {
        "id": "cPBXL-Za9RoN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CombinedModel\n",
        "combined_model = CombinedModel(target_model, trojan_net)"
      ],
      "metadata": {
        "id": "1DsASlpn9I22"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Evaluate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "combined_predictions, true_labels = combined_model.evaluate(\n",
        "    texts,\n",
        "    labels,\n",
        "    trojan_tokenizer,\n",
        "    target_tokenizer,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Evaluate accuracy\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, combined_predictions))"
      ],
      "metadata": {
        "id": "F5JUBd8BMTAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deeb3797-d187-4345-b069-f1793d055746"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-53bcd3f9c4b8>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-15-53bcd3f9c4b8>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item[\"labels\"] = torch.tensor(self.labels[idx])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     19769\n",
            "           1       0.98      0.98      0.98     19769\n",
            "\n",
            "    accuracy                           0.98     39538\n",
            "   macro avg       0.98      0.98      0.98     39538\n",
            "weighted avg       0.98      0.98      0.98     39538\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d9a9f312eb14ab792de52362bcdc6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13ade30e282a4d9f802a461b38563b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d447456d80945b2af5ea9a847a876c9",
              "IPY_MODEL_e31551072b944c46b03e42510793e809",
              "IPY_MODEL_b605b10655784d3b996947708993152f"
            ],
            "layout": "IPY_MODEL_e0446f9fcd88461da1065c7ea7ad572f"
          }
        },
        "171b14e5bd3e4f01b5ffffd07ba82d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c5da370f224e0fa035fddbc8042e13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d447456d80945b2af5ea9a847a876c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c5da370f224e0fa035fddbc8042e13",
            "placeholder": "​",
            "style": "IPY_MODEL_9567725836194275a92a4cbf9e0431dd",
            "value": "model.safetensors: 100%"
          }
        },
        "7bcdb5bac29f4bf7ad67ecde56b94ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbbcda5f302413dad3b42bf66025a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9567725836194275a92a4cbf9e0431dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b605b10655784d3b996947708993152f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171b14e5bd3e4f01b5ffffd07ba82d0b",
            "placeholder": "​",
            "style": "IPY_MODEL_8cbbcda5f302413dad3b42bf66025a3d",
            "value": " 440M/440M [00:05&lt;00:00, 33.6MB/s]"
          }
        },
        "e0446f9fcd88461da1065c7ea7ad572f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e31551072b944c46b03e42510793e809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bcdb5bac29f4bf7ad67ecde56b94ac1",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d9a9f312eb14ab792de52362bcdc6c6",
            "value": 440449768
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}